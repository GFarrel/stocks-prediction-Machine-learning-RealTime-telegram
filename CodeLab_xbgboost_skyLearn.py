# -*- coding: utf-8 -*-
"""Trade_xgboost_balance-the-imbalanced-rf-and-xgboost-with-smote.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/104c__CRrWFkX02VD662g0cdZpl_yGnrb

# Fraud analysis:
### Random Forest, XGBoost, OneClassSVM, Multivariate GMM and SMOTE, all in one cage against an imbalanced dataset.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import warnings

import Utils_plotter

warnings.filterwarnings('ignore')

"""## 1. Supervised learning tests

I will now test a series of different machine learning Models (no Neural Networks!) to see which one performs better, with some optimization here and there.

### 1.1 Data import, quick view and functions
"""

df = pd.read_csv("d_price/FAV_SCALA_stock_history_E_MONTH_3.csv",index_col=False, sep='\t')


"""I wonder if it should be treated as a data series rather than a table... """

numeric_feature_names = ['buy_sell_point','ticker',
                         "Date","Open","High","Low","Close","Volume","per_Close","per_Volume","per_preMarket","olap_BBAND_UPPER","olap_BBAND_MIDDLE",
                         "olap_BBAND_LOWER","olap_HT_TRENDLINE","olap_MIDPOINT","olap_MIDPRICE","olap_SAR","olap_SAREXT","mtum_ADX","mtum_ADXR","mtum_APO",
                         "mtum_AROON_down","mtum_AROON_up","mtum_AROONOSC","mtum_BOP","mtum_CCI","mtum_CMO","mtum_DX","mtum_MACD","mtum_MACD_signal","mtum_MACD_list",
                         "mtum_MACD_ext","mtum_MACD_ext_signal","mtum_MACD_ext_list","mtum_MACD_fix","mtum_MACD_fix_signal","mtum_MACD_fix_list","mtum_MFI",
                         "mtum_MINUS_DI","mtum_MINUS_DM","mtum_MOM","mtum_PLUS_DI","mtum_PLUS_DM","mtum_PPO","mtum_ROC","mtum_ROCP","mtum_ROCR","mtum_ROCR100",
                         "mtum_RSI","mtum_STOCH_k","mtum_STOCH_d","mtum_STOCH_kd","mtum_STOCH_Fa_k","mtum_STOCH_Fa_d","mtum_STOCH_Fa_kd","mtum_STOCH_RSI_k",
                         "mtum_STOCH_RSI_d","mtum_STOCH_RSI_kd","mtum_ULTOSC","mtum_WILLIAMS_R","volu_Chaikin_AD","volu_Chaikin_ADOSC","volu_OBV",
                         "vola_ATR","vola_NATR","vola_TRANGE","cycl_DCPERIOD","cycl_DCPHASE","cycl_PHASOR_inph","cycl_PHASOR_quad","cycl_SINE_sine","cycl_SINE_lead", # "cycl_HT_TRENDMODE",
                         "sti_BETA","sti_CORREL","sti_LINEARREG","sti_LINEARREG_ANGLE","sti_LINEARREG_INTERCEPT",
                         "sti_LINEARREG_SLOPE","sti_STDDEV","sti_TSF","sti_VAR","ma_DEMA_5","ma_EMA_5","ma_KAMA_5","ma_SMA_5","ma_T3_5","ma_TEMA_5","ma_TRIMA_5",
                         "ma_WMA_5","ma_DEMA_10","ma_EMA_10","ma_KAMA_10","ma_SMA_10","ma_T3_10","ma_TEMA_10","ma_TRIMA_10","ma_WMA_10","ma_DEMA_20","ma_EMA_20",
                         "ma_KAMA_20","ma_SMA_20","ma_TEMA_20","ma_TRIMA_20","ma_WMA_20","ma_EMA_50","ma_KAMA_50","ma_SMA_50","ma_TRIMA_50",
                         # TOO MACH null data in this columns ,"ma_DEMA_50","ma_T3_20","mtum_TRIX"
                        #  "ma_WMA_50","ma_EMA_100","ma_KAMA_100","ma_SMA_100","ma_TRIMA_100","ma_WMA_100",
                         "trad_s3","trad_s2","trad_s1","trad_pp","trad_r1","trad_r2",
                         "trad_r3","clas_s3","clas_s2","clas_s1","clas_pp","clas_r1","clas_r2","clas_r3","fibo_s3","fibo_s2","fibo_s1","fibo_pp","fibo_r1","fibo_r2",
                         "fibo_r3","wood_s3","wood_s2","wood_s1","wood_pp","wood_r1","wood_r2","wood_r3","demark_s1","demark_pp","demark_r1","cama_s3","cama_s2",
                         "cama_s1","cama_pp","cama_r1","cama_r2","cama_r3","ti_acc_dist","ti_chaikin_10_3","ti_choppiness_14","ti_coppock_14_11_10",
                         "ti_donchian_lower_20","ti_donchian_center_20","ti_donchian_upper_20","ti_ease_of_movement_14","ti_force_index_13","ti_hma_20",
                         "ti_kelt_20_lower","ti_kelt_20_upper","ti_mass_index_9_25","ti_supertrend_20","ti_vortex_pos_5","ti_vortex_neg_5","ti_vortex_pos_14","ti_vortex_neg_14"]

candle_columns = ["cdl_2CROWS","cdl_3BLACKCROWS","cdl_3INSIDE","cdl_3LINESTRIKE","cdl_3OUTSIDE","cdl_3STARSINSOUTH","cdl_3WHITESOLDIERS",
                         "cdl_ABANDONEDBABY","cdl_ADVANCEBLOCK","cdl_BELTHOLD","cdl_BREAKAWAY","cdl_CLOSINGMARUBOZU","cdl_CONCEALBABYSWALL","cdl_COUNTERATTACK",
                         "cdl_DARKCLOUDCOVER","cdl_DOJI","cdl_DOJISTAR","cdl_DRAGONFLYDOJI","cdl_ENGULFING","cdl_EVENINGDOJISTAR","cdl_EVENINGSTAR",
                         "cdl_GAPSIDESIDEWHITE","cdl_GRAVESTONEDOJI","cdl_HAMMER","cdl_HANGINGMAN","cdl_HARAMI","cdl_HARAMICROSS","cdl_HIGHWAVE","cdl_HIKKAKE",
                         "cdl_HIKKAKEMOD","cdl_HOMINGPIGEON","cdl_IDENTICAL3CROWS","cdl_INNECK","cdl_INVERTEDHAMMER","cdl_KICKING","cdl_KICKINGBYLENGTH",
                         "cdl_LADDERBOTTOM","cdl_LONGLEGGEDDOJI","cdl_LONGLINE","cdl_MARUBOZU","cdl_MATCHINGLOW","cdl_MATHOLD","cdl_MORNINGDOJISTAR","cdl_MORNINGSTAR",
                         "cdl_ONNECK","cdl_PIERCING","cdl_RICKSHAWMAN","cdl_RISEFALL3METHODS","cdl_SEPARATINGLINES","cdl_SHOOTINGSTAR","cdl_SHORTLINE",
                         "cdl_SPINNINGTOP","cdl_STALLEDPATTERN","cdl_STICKSANDWICH","cdl_TAKURI","cdl_TASUKIGAP","cdl_THRUSTING","cdl_TRISTAR","cdl_UNIQUE3RIVER",
                         "cdl_UPSIDEGAP2CROWS","cdl_XSIDEGAP3METHODS"]

columns_valids = numeric_feature_names + candle_columns

df[columns_valids].describe()

#https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/
def plot_confusion_matrix(cf_matrix, path = None, title_str = 'Seaborn Confusion Matrix with labels\n\n' ):
    plt.figure(figsize=(6,6))

    labels = ['True Neg','False Pos','False Neg','True Pos']
    labels = np.asarray(labels).reshape(2,2)

    cm_sum = np.sum(cf_matrix, axis=1, keepdims=True)
    cm_perc = cf_matrix / cm_sum.astype(float) * 100
    annot = np.empty_like(cf_matrix).astype(str)
    nrows, ncols = cf_matrix.shape
    for i in range(nrows):
        for j in range(ncols):
            c = cf_matrix[i, j]
            p = cm_perc[i, j]
            if i == j:
                s = cm_sum[i]
                annot[i, j] = '%.1f%%\n%d/%d' % (p, c, s)
            elif c == 0:
                annot[i, j] = ''
            else:
                annot[i, j] = '%.1f%%\n%d' % (p, c)
    cm = pd.DataFrame(cf_matrix, index=labels, columns=labels)
    cm.index.name = 'Actual'
    cm.columns.name = 'Predicted'

    #ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')
    # ax = sns.heatmap(cf_matrix / np.sum(cf_matrix), annot=True,
    #                  fmt='.2%', cmap='Blues')
    ax =  sns.heatmap(cm, annot=annot, fmt='', cmap='Blues')

    ax.set_title(title_str);
    ax.set_xlabel('\nPredicted Values')
    ax.set_ylabel('Actual Values ');

    ## Ticket labels - List must be in alphabetical order
    ax.xaxis.set_ticklabels(['False','True'])
    ax.yaxis.set_ticklabels(['False','True'])


    ## Display the visualization of the Confusion Matrix.
    # plt.show()
    if path is not None:
        print("plot_confusion_matrix  " + path)
        plt.savefig(path)

def __autopct_fun(abs_values):
    #Source: https://www.holadevs.com/pregunta/64169/adding-absolute-values-to-the-labels-of-each-portion-of-matplotlibpyplotpie
    gen = iter(abs_values)
    return lambda pct: f"{pct:.1f}% ({next(gen)})"
def plot_pie_countvalues(df, colum_count , stockid= "", opion = ""):
    df =  df.groupby(colum_count).count()
    y = np.array(df['Date'])

    plt.figure()
    plt.pie( y , labels=df.index, autopct=__autopct_fun(y),startangle=9, shadow=True)

    name = stockid + "_"+colum_count+"_"+opion
    plt.title("Count times values:\n"+ name)
    plt.savefig("pie_plot_"+ name + ".png")

"""Check for NaNs"""

cleaned_df = df.copy()

# You don't want the `Time` column.
cleaned_df['Date'] = pd.to_datetime(cleaned_df['Date']).map(pd.Timestamp.timestamp)
cleaned_df = cleaned_df[columns_valids]
cleaned_df = pd.get_dummies(cleaned_df, columns = [ 'ticker'])
df = cleaned_df.dropna()

"""WOW! Seriously, no NaNs? 

Ok, let's check for the classes distributions
"""

Y_TARGET = 'buy_sell_point'
Y_target_classes = df[Y_TARGET].unique().tolist()
print(f"Label classes: {Y_target_classes}")
df[Y_TARGET] = df[Y_TARGET].map(Y_target_classes.index)

neg, pos = np.bincount(df[Y_TARGET])
total = neg + pos
print('Examples:\n    Total: {}\n    Positive: {} ({:.2f}% of total)\n'.format(
    total, pos, 100 * pos / total))

plot_pie_countvalues(df,Y_TARGET )

print('Fraud \n',df.Date[df[Y_TARGET]==1].describe(),'\n',
      '\n Non-Fraud \n',df.Date[df[Y_TARGET]==0].describe())

"""Imbalanced dataset. Might be worth to work on upsampling/downsampling of the data, but I will try without it for the moment and hope I get good results. Now let's check which variable is more correlated with the fraudulent activities. """

df.isnull().sum()





Utils_plotter.plot_relationdist_main_val_and_all_rest_val(df,Y_TARGET ,path = "plots_relations\plot_relationdistplot_")
